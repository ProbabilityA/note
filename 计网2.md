[https://github.com/ProbabilityA/note/blob/master/%E8%AE%A1%E7%BD%91.md](https://github.com/ProbabilityA/note/blob/master/%E8%AE%A1%E7%BD%91.md)

## 分层
+ OSI 七层模型
    1. 应用层：通过应用程序间的交互完成特定的网络应用
    2. 表示层：使通信的应用程序能够解释交换数据的含义
    3. 会话层：建立、管理和终止表示层实体之间的会话
    4. 传输层：两台主机进程之间提供通信服务
    5. 网络层：选择合适的网间路由和交换节点
    6. 数据链路层：两台主机之间的数据传输，将网络层传下来的 IP 数据报封装成帧
    7. 物理层：计算机节点之间比特流的透明传送
+ TCP/IP 参考模型
    1. 应用层（应用层、表示层、会话层）：不同协议为不同应用服务
    2. 传输层：端对端主机通信
    3. 网际互连层：相同或不同网络中计算机之间的通信
    4. 网络接入层（数据链路层、物理层）：监视数据在主机和网络之间的交换
+ TCP/IP 五层参考模型
    1. 应用层
    2. 传输层
    3. 网络层
    4. 数据链路层
    5. 物理层
+ 面向连接：通信前要先建立一条通信线路
    - 无连接：不需要先建立通信线路

### 问题
1. **OSI 模型和 TCP/IP 模型的比较**

相同点：都采用了层次结构；都能够提供面向连接和无连接两种通信服务机制。

不同点：

    1. TCP/IP 参考模型没有对网络层进行细分，只是一些概念性的描述；OSI 参考模型对服务和协议作了明确的区分
    2. OSI 参考模型将网络划分为 7 层，实现起来较困难；TCP/IP 模型作为一种简化的分层结构比较容易实现
2. **为什么 TCP/IP 去除表示层和会话层？**

会话层、表示层和应用层是应用程序内部实现的，不同应用程序之间无法实现代码的抽象共享，他们最终的产出是一个应用数据包。

3. **数据如何在各层之间传输？**
    1. 在发送主机端，一个应用层报文被传送到传输层
    2. 传输层把数据分段，将文件分成数据包，并标上顺序号，然后传给网络层
    3. 网络层给每一段加上地址（源 IP 地址，目标 IP 地址），把数据给数据链路层
    4. 数据链路层把当前的 MAC 地址和下一跳的 MAC 地址添加在数据包后面，在数据包前面加上 FCS（帧校验序列）
    5. 物理层把数据链路层的数据帧变成数字信号（比特流）

## HTTP —— HyperText Transfer Protocol
### HTTP
+ 请求过程（<font style="color:rgb(51, 51, 51);">当我们在浏览器的地址栏中输入</font>`<font style="color:rgb(51, 51, 51);">www.baidu.com</font>`<font style="color:rgb(51, 51, 51);">然后回车，具体发生了什么？</font>）
    - DNS解析（域名 -> IP 地址）
        * DNS 是一种应用层协议，它使用的传输层协议是 UDP 协议
        * 解析域名的过程是一个递归查询的过程
            + 先从浏览器的 DNS 缓存查询
            + 如果没有，则再查看本机系统的 DNS 缓存，包括 hosts 文件
            + 如果没有，则向本地域名解析服务系统发起解析域名的请求
            + 如果本地域名解析服务系统也没有，则它会向根域名解析服务系统发起解析根域名的请求
    - （可能）通过 ARP 确定下一跳路由的 MAC 地址，将 IP 数据报封装成帧
    - **建立 TCP 连接（三次握手）**
        * 第一次握手，客户端向服务端发送 SYN 包（SYN = 1，Seq = x），进入 SYN-SENT 状态
        * 第二次握手，服务端收到客户端的 SYN 包，需要对其进行确认并发送自己的 SYN 包，也就是发送一个 TCP 报文段，其中 SYN = 1，ack = x + 1，Seq = y，进入 SYN-RCVD 状态
        * 第三次握手，客户端收到服务端的 TCP 报文段，返回一个 TCP 报文段，ACK = y + 1，Seq = x + 1，然后自己进入 ESTABLISHED 状态
    - 服务端将数据通过 TCP 连接传输给客户端，然后浏览器进行解析、渲染
    - **服务器**在传输完成后（如果没有`Connection: keep-alive`的情况下）请求**关闭 TCP 连接（四次挥手）**
        * 在这个例子中，服务端是主动方，客户端是被动方，但**关闭 TCP 连接的主动方可以是任意一方**
        * 服务端向客户端发送一个带有 FIN = 1 的报文段，此时服务端进入 FIN_WAIT_1 状态，表示服务端没有数据要发送给客户端了
        * 客户端收到了服务端发送的 FIN 报文段，回应一个 ACK 报文段，告诉服务端同意关闭请求，服务端收到后进入 FIN_WAIT_2 状态
        * 客户端发送一个带有 FIN = 1 的报文段，表示客户端没有数据要发送给服务端了，然后客户端进入 LAST-ACK 状态
        * 服务端收到 FIN 报文之后，回应一个 ACK 报文段，然后自己进入 TIME_WAIT 状态**等待 2 个 MSL**
+ 报文格式

```plain
请求报文
<method> <url> <version>
<headers>
CRLF
<entity-body>

响应报文
<version> <status> <reason-phrase>
<headers>
CRLF
<entity-body>
```

+ method 请求方式
    - 在 HTTP 1.0 中为 GET POST HEAD
    - 在 HTTP 1.1 新增 OPTIONS PUT PATCH DELETE TRACE CONNECT
+ headers 请求头
+ CRLF 回车换行（**不是写出来的 CRLF，是真的换行**）
+ status 状态码
+ reason-phrase 可读短语

### HTTP 0.9 单行协议
+ 请求
    - 只支持 GET
    - 不附带版本号
+ 响应
    - 只包含 HTML 响应文档本身
    - 如果出错，HTML 文档中应包含错误信息

```plain
Request:
GET /mypage.html

Response:
<HTML>
这是一个非常简单的HTML页面
</HTML>
```

### HTTP 1.0
+ 报文传输方式：发送请求 -> 等待服务器回复 -> 关闭连接
+ 特点
    - 引入请求头：请求头可以描述更多的元数据，通过请求头可以实现缓存控制
    - 无连接：每个请求都要与服务器建立一个 TCP 连接，处理完成后立刻断开
    - 无状态：不同请求之间没有联系，服务端不知道请求两次之间是否为同一用户
        * 但可以使用 Cookie 和 Session 来维护访问状态
+ 请求方式
    - GET
        * 用于数据的读取操作（**幂等操作**），不应当用于非幂等操作
            + 如`GET /archives`获取档案，无论接口调用多少次都不影响档案结果，是幂等的
            + 如`GET /time`获取服务器时间，无论接口调用多少次不影响时间运行本身，是幂等的
            + 如果`GET /archives`只允许一次性获取档案，也就是接口调用完下次就不会返回一样的档案结果，那就是非幂等的
        * 有 URL 长度限制（浏览器限制）
            + IE 浏览器为 2000 多个字符
            + Chrome 为 8000 多个字符
            + 不同浏览器限制不同
            + **如果超过长度，服务端可能会返回**`414 URI Too Long`
        * 只允许 ASCII 字符
        * 可以被缓存、收藏为书签、记录为浏览器历史记录
    - POST**（可以有请求体）**
        * 提交资源数据给服务端，如 HTML 中的表单
        * 不可以被缓存
        * 报文长度通过`Content-Length`指明
        * 请求体类型通过`Content-Type`指明
    - HEAD
        * 请求获取由 Request-URI 所标识的资源的**响应头**
        * 一般用于下载文件前判断文件大小和文件信息用
+ 缓存控制
    - 强制缓存
        * 通过响应头`Expires`实现，超过指定日期即缓存失效
        * 强制缓存不会像协商缓存一样再请求服务器，第二次请求的状态码为 200
        * 如`Expires: Wed, 21 Oct 2015 07:28:00 GMT`
    - 协商缓存
        * 通过`Last-Modified / If-Modified-Since`控制
        * 客户端第一次请求资源，服务端会响应并返回该资源的最后一次修改时间于`Last-Modified`响应头中，客户端存储该资源和最后修改时间。第二次请求时，客户端会将最后修改时间放在请求头`If-Modified-Since`中，服务端根据这个时间与资源的最后修改时间对比，如果时间一致，则服务端返回 304 状态码表示资源没有更新；如果不一致则返回最新的资源，并返回该资源最后一次修改时间于`Last-Modified`响应头中
+ Cookie
    - Cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器发起请求时被携带并发送到服务器上
    - Cookie 的作用
        * 会话状态管理（用户登陆状态等）
        * 个性化设置（用户自定义设置等）
        * 浏览器行为跟踪（分析用户行为等）
    - 请求头`Cookie`和响应头`Set-Cookie`
        * `Set-Cookie`可以设置 Cookie，并且可以通过`Expires`或`Max-Age`控制指定过期时间，如果不设置则默认为会话期 Cookie，浏览器关闭时会清除（由浏览器决定）
        * `Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;`
+ 缺陷
    - 无法复用连接，每个请求都要新建一个 TCP 连接，TCP 连接的建立和释放比较麻烦，降低了网络利用率
    - 存在队头阻塞问题`head of line blocking`
        * HTTP 1.0 规定下一个请求必须在前一个请求响应到达之前才能发送。假设前一个请求响应一直不到达，那么下一个请求就不发送，同样的后面的请求也给阻塞了。
        * 队头阻塞与长连接、短连接无关，它是由 HTTP 规定报文必须是一发一收导致的

### HTTP 1.1
+ 对 HTTP 1.0 的优化
    - 长连接
        * 通过新增`Connection`请求头，设置`Connection: Keep-Alive`来保持 HTTP 连接不断开，避免每次发送请求都要建立 TCP 连接，提高了网络利用率
        * 当客户端想关闭连接时，可以通过在请求头中携带`Connection: false`来告知服务端关闭连接
    - 管道化`pipelining`
        * 在长连接的基础上，客户端可以一直发送请求，而不用等待服务器响应之前的请求才接着发送，但服务器必须按请求顺序进行响应
        * **实质上将先进先出队列从客户端迁移到了服务端，仍存在队头阻塞问题：如果服务端对第一个请求的响应比较慢，则后面的响应也会被阻塞**
        * 要求
            + 请求必须是幂等请求，如`GET`和`HEAD`，因为意外中断时客户端需要把未收到响应的请求重发，非幂等请求可能会出现非法操作
            + 服务端必须按请求顺序响应
        * 优点
            + 使得客户端能够并行发送请求
        * 缺点
            + 仍存在队头阻塞问题
            + 只适用于幂等操作，非幂等操作可能破坏资源
    - 支持断点续传（文件分块传输）
        * 通过请求头`Range`和`If-Range`、响应头`Content-Range`实现
        * `Range`用于指定请求的数据范围
        * `Range: bytes=0-499`表示第 0 - 499 字节的内容
        * `Range: bytes=0-`表示整个文件下载
        * `Content-Range`表示响应的内容的范围和文件的总大小
        * `Content-Range: 0-499/22400`表示响应第 0 - 499字节的内容，文件总大小为 22400 字节
        * 增强校验
            + 在客户端发起断点续传请求时，URL 对应的文件内容在服务端已经发生变化，这时候续传的数据肯定是错误的，因此需要像缓存控制一样判断资源是否已经发生更新
            + 客户端第一次收到文件块时，需要保存`Etag`或者`Last-Modified`，用于下次断点续传请求附带在请求头`If-Range`中，服务端在响应断点续传请求前会先根据`If-Range`判断文件是否已经发生改变，如果没有改变再继续响应断点续传请求，如果已经改变则要求客户端重新请求
        * 响应码
            + `206 Partial Content`
            + `200 OK`：不支持范围请求的情况下
            + `416 Range Not Satisfiable`：`Range`范围错误
    - 增加`Host`请求头，使得一个服务器能用来创建多个站点（多个虚拟主机）
        * **所有 HTTP 1.1 请求都必须有 Host 请求头**，否则可能会收到`400 Bad Request`
        * 语法：`Host: <host>:<port>`，host 为服务器的域名，port （可选）为端口号
        * **作用：使同一台服务器可以部署多个站点，服务器通过 Host 请求头的内容来判断客户端访问的是哪个站点**
    - 缓存控制
        * 新增`Cache-Control`、`Etag`和`If-None-Match`
        * 强制缓存
            + 通过响应头`Cache-Control`控制，且**包含**`max-age`**时优先级高于**`Expires`
            + `Cache-Control`的取值
                - private：客户端可以缓存
                - public：客户端和代理服务器可以缓存
                - max-age：指定秒数后过期
                - no-cache：需要通过协商缓存来验证是否过期
                - no-store：不可缓存
        * 协商缓存
            + 通过响应头`Etag`和请求头`If-None-Match`来控制，且优先级高于`Last-Modified`和`If-Modified-Since`
            + 客户端第一次请求资源，服务端会响应并返回该资源的唯一标识于`Etag`响应头中，客户端存储该资源和唯一标识。第二次请求时，客户端会将资源唯一标识放在请求头`If-None-Match`中，服务端根据这个标识寻找资源，如果文件没有更新，则服务端返回 304 状态码表示资源没有更新；如果不一致则返回最新的资源，并返回该资源最新的标识于`Etag`响应头中
            + **解决了 HTTP 1.0 中协商缓存的缺陷**：文件修改后撤销，修改日期发生改变导致缓存失效的问题
+ 对 POST 的拓展
    - **有可能**发送两个 TCP 报文（比如跨域的时候，由浏览器决定）
        * 第一次发送一个`OPTIONS`请求，只发送`Headers`，服务端返回`100 Continue`
        * 第二次再发真实的`POST`请求
    - **分块传输**
        * 大文件的下载、有时候服务端无法确定响应消息的大小
        * 通过响应头`Transfer-Encoding: Chunked`控制
+ 新增请求方法
    - OPTIONS
        * 客户端判断服务器的能力
        * 常见应用场景
            + 跨域请求前（服务器返回`Access-Control-*`一系列响应头）
    - PUT**（可以有请求体）**
        * 请求更新资源，如果没有则创建
        * 如果更新了资源，服务端应返回`200`或`204`，如果创建了新资源，服务端应返回`201 Created`，如果不能创建或更新，应该返回合适的错误响应
    - DELETE
        * 删除对象，无请求体
    - PATCH
        * 局部更新（PUT 是全量更新）
    - TRACE
        * 客户端的请求经过防火墙、代理等中间节点，中间节点会修改原始的 HTTP 请求，TRACE 方法允许客户端看到请求在最后到达服务器时变成了什么样子
        * 用于验证请求是否如愿穿过请求 / 响应链
        * 存在危险
            + 由于该方法原样返回客户端提交的任意数据，因此可以用来进行跨站脚本`XSS`攻击（将恶意代码注入到网页，使用户加载）
    - CONNECT
        * 建立代理连接
+ 缺陷
    - 连接无法复用：虽然`keep-alive`可以复用一部分连接，但是对于同一台服务器，在域名分片的情况下仍需要建立多个连接
    - 仍存在头部阻塞问题：虽然提出了管道化，但是在服务端仍存在头部阻塞问题，因为管道化要求必须按照请求顺序进行响应
    - 请求头和响应头开销大：每次请求 Headers 基本不怎么变化，而且 Headers 携带的内容越来越多
    - 安全因素：只使用 HTTP 1.x 协议时传输内容都是明文，客户端和服务端都无法验证对方的身份

### HTTP/2
+ 目标：专注于性能，最大的目标是在用户和网站间只用一个连接
+ 二进制传输
    - 将请求和响应数据分割为更小的帧，并采用二进制编码
    - 可以有多个流，每个流有一个唯一的整数 ID，可以承载双向的消息
    - **多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装**
+ 多路复用
    - 同域名下所有通信在单个 TCP 连接完成（但是如果存在丢包等网络问题，有可能表现不如 HTTP 1.1，因为 HTTP 1.1 采用多个 TCP 连接，网络问题影响的只是其中某个连接；而 HTTP/2 下整个 TCP 都要开始等待重传，导致后面数据被阻塞，即 TCP 层面的队头阻塞）
    - **单个连接可以承载任意数量的双向数据流（真正的并行传输），多个帧可以乱序发送**
    - 可以为流设置优先级，客户端和服务端可以在处理不同的流时采取不同的策略
+ 头部压缩
    - 在客户端和服务端维护首部表，对于相同的数据不再通过每次请求和响应发送
    - 每个新的首部键值对要么追加到当前表的末尾，要么替换表中之前的值
    - ![](https://cdn.nlark.com/yuque/0/2022/png/1732113/1641709999035-253fa0f1-7e03-4075-ae46-b564b71c7810.png)
+ 服务器推送
    - 服务端在客户端没有明确请求下抢先地推送一些网站资源给客户端
    - 比如浏览器请求`/index.html`，服务端推送`/index.html`和`styles.css`
    - 浏览器可以通过发送`RST_STREAM`帧来拒收
+ SPDY
    - HTTP/2 基于 SPDY 3，特性与 HTTP/2 差不多

### HTTP/3
+ HTTP/3 使用基于 UDP 协议的 QUIC 协议实现
+ QUIC
    - 通过 UDP 建立连接，在应用层保证传输的可靠性
    - 减少了 TCP 三次握手和 TLS 握手时间
+ 避免队头阻塞的多路复用（UDP 协议本身的优势）
    - 流之间是相互独立的，Stream 2 丢了一个 Packet 并不影响其他的 Stream，不存在 TCP 情况下的队头阻塞
+ 使用 64 位的连接 ID 作为标识
    - 利用连接 ID 的优势，使得用户在迁移网络环境的情况下可以保持连接，例如从蜂窝网络连接迁移到 Wi-Fi 连接，原本的 QUIC 连接依旧可以被保持
+ 0-RTT
    - 传输层使用 UDP 协议，不需要握手，所以 0-RTT 就可以开始传输数据
    - QUIC 把传输和加密的握手合并为一个，以 1-RTT 建立连接，后续的连接可以通过上一次连接时缓存的信息（TLS 1.3 DH 公钥、传输参数等）直接在一个经过认证且加密的通道传输数据（0-RTT）
+ 加密认证报文
    - 加密层使用 TLS 1.3，没有办法或机制避免使用 TLS 连接
+ 向前纠错 `FEC Forward Error Correction`
    - 一段数据被切分为 10 个包，依次对每个包进行异或运算，结果作为 FEC 包与数据包一起被传输，如果有一个数据包丢失，则可以根据剩余的 9 个包和 FEC 包推算出丢失的那个包的数据（现阶段带宽不是网络瓶颈，往返时间才是，所以新的网络传输协议可以适当增加数据冗余，减少重传操作）

### 问题
+ 哪种请求方法可以有请求体？
    - POST PUT
+ HTTP 1.0 支持哪些请求方式？HTTP 1.1 呢？
    - HTTP 1.0 GET POST HEAD
    - HTTP 1.1 OPTIONS PUT DELETE TRACE
+ HTTP 1.0 有哪些特点？有哪些缺陷？
    - 特点：引入请求头来描述元数据 / 无连接 / 无状态
    - 缺陷：网络利用率低 / 存在队头阻塞问题
+ 什么是幂等操作？如何保证业务幂等性？
    - 执行相同的操作，得到相同的结果
    - 保证幂等性——以支付订单为例
        * 将并行事件串行化（加锁，乐观锁或悲观锁）
            + 支付前判断订单是否已支付
        * 分布式锁
            + 支付前先申请 Token，服务端存储 Token
            + 再带着 Token 去支付，如果 Token 不存在则说明支付已完成
        * 消息队列缓冲
            + 订单的支付请求进入到消息队列中，异步任务过滤队列中重复的支付请求
+ 什么是长连接？什么是短连接？
    - 长连接指 HTTP 1.1 中一个 TCP 连接可以被多个请求所复用，直到浏览器请求关闭连接
    - 短连接指 HTTP 1.0 中一个请求必须新建一个 TCP 连接，请求完成后立刻关闭连接
+ HTTP 1.0 中缓存控制的缺陷在哪？HTTP 1.1 是怎么解决的？
    - 在协商缓存时，如果文件修改了一个字符，又撤销了这个字符的修改，最后修改时间依然会改变，这时候即使资源没有变化但是也会使浏览器缓存失效
    - 通过响应头`Etag`和请求头`If-None-Match`解决
+ HTTP 队头阻塞是什么？怎样解决？
    - HTTP 1.0 表现为……（客户端阻塞）
    - HTTP 1.1 管道化场景下表现为……（服务端阻塞）
    - 解决方式
        * 并发 TCP 连接（Chrome 的做法）
        * 域名分片
            + Chrome 对一个域名最多采用 6 个 TCP 连接，如果域名分片为多个域名则可以支持更多的 TCP 连接，比如`baidu.com`再分为`a.baidu.com`、`b.baidu.com`等
        * HTTP 2.0 中多路复用解决
            + HTTP 2.0 将多个请求复用同一个 TCP 通道，通过二进制分帧并且给每个帧打上流的 ID 去避免依次响应的问题，对方接收到帧之后根据 ID 拼接处流，这样可以做到乱序响应从而避免队头阻塞
+ HTTP 1.1 中范围请求（断点续传）和分块传输编码的区别？
    - 前者通过`Range`等请求头控制，后者通过`Transfer-Encoding`控制
    - 前者主要解决了大文件传输请求中断可以接着传输的问题，后者主要解决服务端无法确定数据的精确大小或请求未能完全处理的问题
    - **两者是兼容的，可以单独或搭配使用**
+ HTTP 1.1 中 Host 请求头的作用？
    - 使得同一台服务器可以部署多个站点，服务器通过 Host 请求头来判断客户端访问的是哪个站点
+ Chrome 浏览器中的资源并行加载是由 HTTP 1.1 的管道化特性实现的吗？
    - 不是，实质上是 Chrome 浏览器同时打开了多个 TCP 连接实现数据并行传输

## TCP
+ 三次握手 & 四次挥手（看上面 HTTP 部分）

![](https://cdn.nlark.com/yuque/0/2022/webp/1732113/1641539671074-086d0a5d-f4b4-490e-8317-c21207c15c84.webp)

![](https://cdn.nlark.com/yuque/0/2022/webp/1732113/1641539693543-9660fab5-0a93-4542-be23-44acb58ed3d8.webp)

![](https://cdn.nlark.com/yuque/0/2022/webp/1732113/1641539702121-980707e3-26cf-4f49-8d6b-fb4b2890bc6f.webp)

![](https://cdn.nlark.com/yuque/0/2022/webp/1732113/1641539708837-719faf63-af68-40f2-95f8-fdd721fd2534.webp)

### 特点、使用场景
+ 面向连接：数据传输前要建立连接，数据传输结束要关闭连接（三次握手四次挥手）
+ 流量控制：滑动窗口
+ 拥塞控制：慢开始、拥塞避免、快重传、快恢复
+ 可靠性：检验和、序列号顺序控制、确认应答、重发机制、连接管理、窗口控制
+ 应用程序传送的数据块太长，TCP 会将其划分短一些再传送

### 可靠性
#### 三次握手
三次握手保证双方能在连接上正常通信

#### 包编号、累积确认
每个 TCP 报文段都有一个编号，保证接收端实体按序接收包并能够向发送方确认已接受的包编号

#### 滑动窗口（流量控制）
在发送方开辟一个缓存空间，发送方收到确认之前，必须在缓冲区保留已发送的数据，如果收到确认应答即可从缓冲区清除。

窗口的大小指无需等待确认应答而可以继续发送数据的最大值，**通常由接收端的缓冲能力决定，通过 TCP 头部字段 Window 控制**。

![](https://cdn.nlark.com/yuque/0/2022/png/1732113/1641727177955-c47e4692-27bd-475d-8903-23ea864670f1.png)

#### 拥塞控制
+ 流量控制是避免发送方的数据填满接收方的缓存
+ 拥塞控制是避免发送方的数据造成网络拥堵
+ 拥塞窗口 cwnd 没有拥塞就增大，拥塞就减小，拥塞的判断标准是发生超时重传
+ AIMD
    - 慢开始（指数增长）
    - 拥塞避免（超过 ssthresh，cwnd 每次增加 1）
    - ![](https://cdn.nlark.com/yuque/0/2022/png/1732113/1641801882973-cf0f6afc-8ce3-427c-ad94-8548bd98c690.png)
    - 拥塞发生，将慢开始门限 ssthresh 重置为当前 cwnd / 2
        * 超时重传，将 cwnd 重置为 1（但是会导致突然减少数据流）
        * 快速重传，将 cwnd 设置为原来的一半
    - 快速恢复
        * 从慢开始门限直接进入拥塞避免阶段
        * ![](https://cdn.nlark.com/yuque/0/2022/png/1732113/1641802221526-f5f3ec4e-f41e-4a51-99e1-3739bf5ecade.png)
+ TCP Tahoe
    - 慢开始
    - 拥塞避免
    - 快重传
    - cwnd = 1（见上图）
+ TCP Reno
    - 见上图，区别是快重传后 cwnd = 慢开始门限，然后进入拥塞避免
+ TCP Vegas（关注延迟而不是丢包）
    - 慢开始
    - 自适应增长
        * Vegas 定义了两个阈值α和β，当 queueSize > β 时，拥塞窗口减小，当 α <= queueSize <=β 时，拥塞窗口不变，当 queueSize < α 时，拥塞窗口增加。α和β的通常是一个较小的取值，例如α=1, β=3，则表示在传输过程中最多只使用 2 个 buffer。这样可以保证系统容量评估得尽可能精准，但是带来的坏处是需要进行非常频繁的调节，适当增大α,β的范围可以减少频繁的调整 cwnd
+ TCP New Reno（考虑了一次丢失多个报文的情况）
    - TCP Reno 收到一个新的 ACK 就退出快速恢复状态，而 TCP New Reno 只有当所有报文都被应答后才退出快速恢复状态
+ TCP BIC
    - 通过二分搜索的思想找到最合适的拥塞窗口
    - 指数增长到丢包
    - 丢包窗口为最大窗口值，最小窗口值取最大最小值的和的一半
    - 不断二分靠近最大值
    - 接近最大值时重新指数增长
    - ![](https://cdn.nlark.com/yuque/0/2022/jpeg/1732113/1641803469049-ff4a360a-b186-4aa0-88b4-7cdbc781e26d.jpeg)
+ TCP CUBIC
    - 采用三次函数来取中间值，快速收敛来使用更多带宽
+ TCP WestWood
    - 适用无线网络
    - 基于 TCP New Reno，通过包测量确认一个合适的发送速度，改进慢启动算法为敏捷探测
+ TCP BBR
    - 基于模型主动探测
    - 慢启动
    - 排空阶段（等 RTT 稳定，可以认为缓存区排空）
    - 带宽探测阶段
    - 延时探测阶段
    - 目标是尽量不用缓冲区，在堆积之前开始机动

#### 重传机制
+ 超时重传
    - 发送数据时设定一个定时器，超过指定时间后如果没有收到对方的 ACK 报文则重发该数据
    - 超时重传时间`RTO`应略大于`RTT`，太大导致重发效率不高，太小导致可能没有丢包就重发，增加网络拥塞
+ 快速重传
    - 收到三个同样的 ACK，则重传 ACK 编号对应的 TCP 报文段
    - ![](https://cdn.nlark.com/yuque/0/2022/png/1732113/1641726499854-703c7a86-3c26-473d-90b0-1c2b045dfbe3.png)
+ `SACK`选择性确认
    - 在 TCP 头部的`SACK`字段说明哪些数据收到了，哪些数据没收到，只重传丢失的数据
+ `D-SACK Duplicate ACK`
    - 告诉发送方哪些数据被重复接受了

#### ARQ 协议（自动重传请求）
+ 停等 ARQ 协议
    - 发送窗口和接收窗口都为 1，发送方发送一个帧后必须接受到一个 ACK 帧才能发送下一个
+ 连续 ARQ 协议
    - 回退 N 帧 ARQ 协议
        * 接收端丢弃从第一个没有收到的数据包开始的所有数据包
        * 发送端收到 NACK 后，从 NACK 指明的数据包开始重新发送

### 糊涂窗口
+ 接收方窗口较小，但是仍告诉发送方导致发送方发送少量数据，造成资源浪费（TCP + IP 头有 40 多字节，但只传输几个字节的数据）
+ 解决方法
    - 接收方不通告小窗口给发送方，直接通告窗口为 0
    - 发送方避免发送小数据（延时处理），使用 Nagle 算法，满足以下两条件之一才可以发送数据
        * 窗口大小 >= MSS 或数据 >= MSS
        * 收到之前发送数据的 ACK 报文
    - 对于交互性较强的程序如 Telnet 或 ssh，需要关闭 Nagle 算法

### 窗口关闭
+ 窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止。
+ 潜在危险
    - 接收方处理完数据后，会发送一个窗口非 0 的 ACK 报文，如果这个 ACK 报文在网络中丢失了，那发送方就会一直等待接收方的非 0 窗口通知，接收方就会一直等待发送方的数据，造成死锁
+ 解决方式
    - TCP 连接一方收到对方的零窗口通知，就启动持续计时器，如果计时器超时就会发送窗口探测报文

### KeepAlive
+ 默认情况下为关闭，可以被上层应用打开
+ 作用
    - 探测连接的对端是否存活
    - 防止中间设备（防火墙等）因超时删除连接相关的连接表
+ 一般在服务端启用，防止已失效连接占据服务端资源
+ 参数	
    - `tcp_keepalive_time`默认值 7200 s，即两小时无通信后发送心跳
    - `tcp_keepalive_probes`默认值为 9，即没有收到对方确认，**继续**发送保活探测包的次数
    - `tcp_keepalive_intvl`默认值为 75 s，继续发送保活探测包的发送频率
    - **Java 只能做到打开 KeepAlive 功能，参数调整依赖于 Java 自己读取系统配置**
+ 不是 TCP 规范的一部分，消耗了不必要的带宽

### FastOpen
+ 背景：相比 UDP，TCP 需要三次握手，多了一个 RTT 的时延（第三次握手可以携带数据），但是额外多出的这一个 RTT 对应用的时延仍有非常大的影响
+ TFO（TCP Fast Open）允许服务器和客户端在握手阶段交换数据，节省这一个 RTT 的时延
+ 过程
    - 经历一个普通的三次握手获取 Fast Open Cookie
    - 之后如果还需要建立 TCP 连接，由客户端发送一个带有 **SYN、Fast Open Cookie 和数据**的 TCP 报文段
    - 服务端**校验 Fast Open Cookie 的有效性**
        * 如果有效，则向用户发送 SYN-ACK 包，然后传输层会把数据交给应用层，在用户回复 ACK 之前便可以向用户传输数据，
        * 如果无效，则丢弃数据，并直接返回一个 SYN-ACK 包（第二次握手）
    - 客户端发送 ACK 包来确认服务端的 SYN 和数据（第三次握手），如果客户端的 SYN 包中的数据没有被服务器确认，则客户端会在这个 ACK 包中重传对应的数据
+ TFO Cookie
    - 可重复使用，但是有**有效期**
    - 不提供身份认证，目的不是抵挡中间人
    - Cookie 与源地址有关

### 问题
+ 为什么建立 TCP 连接要三次握手？（每一次握手的作用是什么？）
    - 从收发能力来看
        * 第一次：让服务端知道客户端的发送能力、服务端的接收能力正常
        * 第二次：让客户端知道自己和服务端的发送能力和接收能力都正常（此时服务端不能确认客户端的接收能力是否正常）
        * 第三次：为了让服务端知道客户端的接受能力（和自己的发送能力）是正常的
    - 序列号可靠同步
        * 如果只有两次握手，则服务端无法确定客户端是否已经接收到了自己发送的初始序列号，如果第二次握手的报文段丢失，则客户端无法知道服务端的初始序列号，那么 TCP 的可靠性就无从谈起
    - 阻止重复历史连接的初始化
        * 为了防止已失效的连接请求报文段突然传送到服务端。服务端收到已失效的连接请求报文段会向客户端发送确认收到的 TCP 报文段（第二次握手），**如果没有第三次握手，此时服务端会认为新的连接已经建立了，但是客户端不会理睬服务端的第二次握手**，因为对它来说这个连接请求已经失效了。**这就会造成服务端以为连接已经建立，然后一直在等待客户端发送数据，造成资源的浪费。**
    - ~~<font style="color:#8C8C8C;">安全问题</font>~~（**别说，说了的话要准备好回答 DDoS 系列的问题**）
        * ~~<font style="color:#8C8C8C;">如果采用两次握手，会放大 DDoS 攻击</font>~~
+ 第一次握手可以携带数据吗？
    - 不可以，只有第三次握手可以携带数据。前两次握手无法确认双方的收发能力是否正常，如果携带数据就需要缓存空间来存放数据，这时候会放大 DDoS 攻击
+ 什么是半连接队列？全连接队列？
    - 半连接队列是服务端存放状态为 SYN-RCVD 的连接的队列
    - 全连接队列是服务端存放状态为 ESTABLISHED 的连接的队列，如果满了可能会丢包
+ 为什么握手需要三次，挥手需要四次？
    - 因为握手的时候，第二次握手把 SYN 和 ACK 合到同一个报文段，但是挥手的时候，在主动关闭方发送 FIN 报文段后，**接收方可能还要继续发送数据**，不能立即关闭接收方到主动关闭方的数据通道，因此只能先发送 ACK 报文进行确认，等到接收方数据发送完毕再发送 FIN 报文请求关闭数据通道
+ 为什么要有 TIME-WAIT 状态（TIME-WAIT 状态为什么要等待 2 MSL）
    - MSL 是报文在网络中的最大生存时间
    - 第四次挥手时的 ACK 包可能是不可达的，被动关闭方如果接收不到这个 ACK 包，主动方就必须要重新发送。**2 MSL 包含了 ACK 包发送给被动方的 1 个 MSL 和被动方重传 FIN 包的 1 个MSL**，也就是说等待 2 个 MSL 仍没有收到新的 FIN 包就**可以说明被动方已经收到了第四次握手的 ACK 包**。
+ 如果没有收到确认应答，数据一定丢失了吗？
    - 不一定，可能是数据对方已经收到，只是 ACK 包在途中丢失，可以通过后续接收方累积确认解决。这种情况可能也导致发送端误以为数据没有达到目的地而重发数据

## UDP
### 特点、使用场景
+ 面向无连接：发送数据前不需要握手
+ 有单播、多播、广播的功能
+ 面向报文：UDP 对应用层交付的报文不合并也不拆分，因此应用程序需要选择合适大小的报文
+ 不可靠性：没有拥塞控制，不保证数据完整性，不保证数据顺序
+ 头部开销小，实时性高
+ 对**高速传输和实时性要求较高的通信、包总量较少的通信（DNS、SNMP）**或**广播通信**

### KCP
+ 使 UDP 变得可靠
+ 以 10% - 20% 的带宽浪费代价换取比 TCP 快 30% - 40% 的传输速度
+ 特点
    - RTO（重传超时时间）不翻倍（1.5 倍），TCP 为 2 倍
    - 选择性重传（只重传真正丢失的数据），TCP 为最小序号丢失以后的数据
    - 快速重传，ACK = 1，3，4，5，当收到 ACK = 3 时，直到 2 被跳过 1 次，收到 ACK = 4 时，知道 2 被跳过一次，此时直接重传 2 号包
    - 可选的延迟 ACK
    - 可选的非退让流控

## 提高网络利用率
### 粘包、拆包
+ 什么是粘包
    - 发送方发送的若干包数据到接收方接收时粘成一个包，从接收缓冲区看后一包数据的头接着前一包数据的尾（TCP 是面向字节流的协议，就是没有界限的一串数据）
+ 为什么 UDP 没有粘包，而 TCP 有
    - 因为 UDP 有消息保护边界，而 TCP 没有，因此需要应用层协议自己设计消息的边界，即消息帧
+ 粘包发生场景
    - 接收端没有及时读取缓冲区的数据，或数据发送过快导致接收端缓冲区堆积
    - TCP 将多个请求合并为同一个请求进行发送（要发送的数据太小，TCP 将多次写入缓冲区的数据一次性发出去）
+ 拆包发生场景
    - 如果数据包太大，超过`MSS`的大小，就会被拆成多个包发送
+ 常见解决方案
    - 发送端将每个包封装成固定的长度
    - 在每个包的末尾使用固定的分隔符（FTP 协议使用`\r\n`）
    - 将消息分为头部和消息体，头部中保存整个消息的长度，读到足够长度的消息之后才算读到一个完整的消息（对应拆包情况）
    - 通过自定义协议进行粘包和拆包的处理
+ Netty 对粘包和拆包问题的处理
    - `LineBasedFrameDecoder`：以行为单位进行数据包的解码
    - `DelimiterBasedFrameDecoder`：以特殊符号为分隔符进行数据包的解码
    - `FixedLengthFrameDecoder`：以固定长度进行数据包的解码
    - `LengthFieldBasedFrameDecoder`：适用于消息头包含消息长度的协议（常用）

### Nagle 算法
+ 目的：避免发送小的数据包，延时发送
+ 只允许一个未被 ACK 的包存在于网络，实质上是一个扩展的停-等协议，但是**如果对端 ACK 回复很快的话，Nagle 算法实质上不会拼接太多的数据包**，虽然避免了网络拥塞，但是网络利用率依然很低
+ 算法规则
    - 包长度达到 MSS，允许发送
    - 包含 FIN，允许发送
    - 设置了 TCP_NODELAY（即关闭 Nagle 算法）
    - 未设置 TCP_CORK 选项时，小数据包被确认则允许发送
    - 发生了超时（一般为 200 ms ）时立即发送

### 延时确认
+ 目的：减少网络中传输大量的小报文（针对 ACK 报文）
+ 发送 ACK，如果没有报文要发送给对方，则等待下一个 200 ms 超时的时候发送 ACK，如果有则马上发送；当有两个未确认的包需要确认时，会立刻发送 ACK
+ **与 Nagle 算法一起使用的问题**
    - **客户端发送一个数据包，由于延时确认，服务端不马上确认，客户端启用 Nagle 算法，要等到上一个数据包的确认再发送下一个数据包，会造成客户端和服务端相互等待，直到超时**

## HTTPS —— HTTP over TLS / SSL
+ 对称加密
    - 计算量小，加密速度快，效率高
    - 双方需要使用相同的密钥，无法避免密钥传输，因此安全性得不到保证
+ 非对称加密
    - 加解密使用不同的钥匙，私钥不通过网络传输，安全性高
    - 但计算量大，加解密速度慢
+ 因此客户端生成随机码 KEY，并通过服务端的数字证书对随机码 KEY 进行非对称加密并传输给服务端，服务端解密后便得到随机码 KEY，双方再使用随机码 KEY 对消息进行对称加密

![](https://cdn.nlark.com/yuque/0/2022/jpeg/1732113/1641717336577-ece7b6fb-59f2-4c4f-bddd-87d533e927ca.jpeg)



## SSL / TLS
+ SSL 1.0 - 3.0 / TLS 1.0 - 1.1 存在安全问题
+ TLS 1.2
    - RTT 消耗时间
        * TCP 握手 1RTT（1.5RTT）
        * TLS Hello 1RTT（拿到证书）
        * TLS 加密 1RTT（用公钥对随机码加密）
        * 真正请求 1RTT（通过最终密钥对称加密消息并传输）

![](https://cdn.nlark.com/yuque/0/2022/jpeg/1732113/1641719302919-7c9f0691-b6de-4a7e-91de-db831447c616.jpeg)

+ DH 算法（密钥交换算法）
    - 素数 p q，公钥 A B
    - 甲选择素数 p，底数 g，随机数 a，计算出公钥 A = g^a mod p
    - **甲发送 p g A 给乙**
    - 乙选择随机数 b，计算出公钥 B = g^b mod p **并发送给甲**，同时计算出 s = A^b mod p
    - 甲算出 s =  B^a mod p
    - 双方通过 s 对称加密消息，只有两次发送
+ TLS 1.3
    - 减少握手延迟
        * 只能使用 DH 算法加密
        * 握手时直接带上 public key
    - 加密更多握手消息
        * 对 Hello 包进行签名保证不被篡改
    - 0-RTT
        * 可能存在重放攻击，一般只用于`GET /`

## 序列化协议
### JSON
+ 特点
    - 字符集为`UTF-8`
    - 字符串和`Object`的键必须使用双引号
+ 数据类型
    - number
    - boolean
    - string
    - `null`
    - `[]`
    - `{}`

### Protobuf
### JDK
### Hessian
### thrift
## NAT 协议
+ NAT 网络地址转换
    - SNAT 修改源地址（比如`192.168.1.123`访问`223.5.5.5`，如果不修改源地址则`223.5.5.5`无法将响应传回给`192.168.1.123`，因为不知道这一个内网该怎么走）
        * 方式
            + 静态地址转换
            + 动态地址转换（常用）
        * 应用
            + 内网用户访问外网资源
    - DNAT 修改目的地址
        * 应用
            + 内部用户对外提供服务
            + 端口转发
        * 外部主动发起连接，由带有公网 IP 的网关来替代外部接受链接，然后在内部做地址转换

## 路由选择协议
+ 内部网关协议
    - 自治系统内部使用
    - RIP**（基于 UDP）**
        * 一路由器到直接连接的网络的距离为 1，经过一个路由器跳数增加 1
        * 工作原理：相邻路由器定期交换各自的路由信息
        * 缺点
            + 好消息传得快，坏消息传的慢
            + 只适合小型同构网络（距离小于 15）
            + 每隔 30 秒一次路由信息广播，可能形成网络风暴
    - OSPF**（采用 IP 数据报）**
        * 使用 Dijkstra 迪杰斯特拉算法
        * 向本自治系统所有路由器发送信息，信息是与本路由器相邻的所有路由器的链路状态
        * 链路状态发生变化时才发送
+ 外部网关协议
    - 不同自治系统之间
    - BGP**（基于 TCP）**
        * 并非找到最好地路由，力求找到能到达目的网络的路由
        * 采用**路径向量路由**选择协议
    - ISIS

## 差错检测
### 奇偶校验
+ 偶校验方案：附加一个比特，使得这所有比特中 1 的总数是偶数
+ 上述方式只能发现奇数个比特差错
+ 改进：二维单比特奇偶校验
    - d 个比特被分成 i 行 j 列，对每行每列计算奇偶值，通过产生的 i + j + 1 个比特来检测和纠错，可以检测和纠正 1 比特的错误

### 校验和
+ TCP 和 UDP 对所有字段（首部和数据）计算校验和
+ IP 协议只对头部计算校验和

### 循环冗余检测 CRC
